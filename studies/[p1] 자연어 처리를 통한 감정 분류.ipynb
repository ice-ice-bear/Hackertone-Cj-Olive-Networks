{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 코드는 Tensorflow 공식 문서를 참고하여 일부를 수정한 코드입니다.\n",
    "# https://www.tensorflow.org/text/tutorials/transformer\n",
    "\n",
    "\n",
    "# Transformer는 self attention에서 모든 위치의 단어가 한번에 입력으로 들어갑니다.\n",
    "# 이 때 각 단어의 위치 정보를 함께 나타내기 위해 Positional Embedding이라는 장치를 통해\n",
    "# 각 단어를 새롭게 임베딩 합니다.\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, input_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "        self.pos_encoding = self.positional_encoding(length=input_length, depth=d_model)\n",
    "\n",
    "    def positional_encoding(self, length, depth):\n",
    "        depth = depth / 2\n",
    "\n",
    "        positions = np.arange(length)[:, np.newaxis]       # (seq, 1)\n",
    "        depths = np.arange(depth)[np.newaxis, :] / depth   # (1, depth)\n",
    "\n",
    "        angle_rates = 1 / (10000 ** depths)       # (1, depth)\n",
    "        angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "        pos_encoding = np.concatenate(\n",
    "            [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "            axis=-1\n",
    "        )\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# 하나의 Transformer 인코더는 여러개의 인코더 Layer가 일렬로 연결된 구조입니다.\n",
    "# 이 클래스는 그 하나의 Layer를 의미합니다.\n",
    "# 여기서 Transformer의 핵심인 Muti Head Attention이 계산됩니다.\n",
    "class TransformerEncoderLayer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        d_ff,\n",
    "        dropout_rate\n",
    "    ):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        # Attention\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            key_dim=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        # Feed-Forward\n",
    "        self.ff = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_ff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.ff_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        # Attention\n",
    "        attn_out = self.mha(x, x)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        out = x + attn_out\n",
    "        out = self.layer_norm(out)\n",
    "\n",
    "        # Feed-Forward\n",
    "        ff_out = self.ff(out)\n",
    "        out = out + ff_out\n",
    "        out = self.ff_norm(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Transformer 인코더 layer를 복수로 쌓을 수 있게 만든 모델 클래스입니다.\n",
    "# 이 모델에 Positional Embedding과 Classifier를 함께 추가하여 분류 모델로 사용합니다.\n",
    "class TransformerEncoder(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        d_ff,\n",
    "        vocab_size,\n",
    "        num_classes,\n",
    "        input_length,\n",
    "        dropout_rate=0.0,\n",
    "    ):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        # Embedding\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size, d_model, input_length)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        # Encoder Layers\n",
    "        self.enc_layers = [\n",
    "            TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                num_heads=num_heads,\n",
    "                d_ff=d_ff,\n",
    "                dropout_rate=dropout_rate\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        # Embedding\n",
    "        out = self.pos_embedding(x)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Encoder Layer\n",
    "        for enc_layer in self.enc_layers:\n",
    "            out = enc_layer(out)\n",
    "\n",
    "        # Classifier\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    tokenizer_layer,\n",
    "    TransformerEncoder(\n",
    "        num_layers=2, # 인코더 layer는 두개 사용\n",
    "        d_model=embedding_dim, # 임베딩된 단어 벡터의 차원\n",
    "        num_heads=4, # Multi Head Attention에서 head의 개수\n",
    "        d_ff=256, # Feed-Forward layer에서 사용할 차원 수\n",
    "        vocab_size=tokenizer_layer.vocabulary_size(), # Tokenizer가 찾은 전체 토큰 개수\n",
    "        num_classes=num_classes, # 클래스 개수\n",
    "        input_length=max_sequence_length, # 입력 문장의 최대 토큰 수. 앞서 50개로 지정했었습니다.\n",
    "        dropout_rate=0.05\n",
    "    )\n",
    "])\n",
    "\n",
    "# 모델을 구성하는 layer들의 요약 정보를 확인합니다.\n",
    "transformer_model.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "num_epochs = 5\n",
    "transformer_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = transformer_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(num_epochs)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
