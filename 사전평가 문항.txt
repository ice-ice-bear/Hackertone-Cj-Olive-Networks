1.  이미지 분류를 위한 심층 신경망에 주로 사용되는 활성화 함수(Activation Function)는? 
① sigmoid
② linear
③ relu
④ tanh

2. 적응적 학습률을 사용하지 않는 옵티마이저는?
① RMSprop
② Adam
③ Adagrad
④ SGD

3. 가중치가 없으며 특성 맵의 가로세로 크기를 줄이는 역할을 수행하는 것은?
① 풀링 (Pooling)
② 필터 (Filter)
③ 드롭 아웃 (Drop Out)
④ 스트라이드 (Strides)

4. 합성곱 층의 필터가 입력 위를 이동하는 간격을 조절하는 Conv2D 클래스의 매개변수는? 
① kernel_size
② padding
③ strides
④ size

 5. 딥러닝의 모델 훈련 중 과대적합(overfitting)을 해결하기 위해 Geoffrey Hinton 교수가 제안한 방법은?
① dropout
② skip-connection
③ max pooling
④ average pooling

6. 어떤 합성곱 층이 컬러 이미지에 대해 5개의 필터를 사용해 same padding으로 합성곱을 수행한 후, (2, 2) 풀링 층을 통과한 특성 맵의 크기가 (8, 8, 5)이다. 이 경우, 주입된 입력의 크기는?
① (32, 32, 3)
② (32, 32, 5)
③ (16, 16, 3)
④ (16, 16, 5)

7. (28 X 28 X 1)의 입력 영상에 32개의 3 X 3 컨볼루션 필터를 stride 1로 적용했다면, 해당 레이어에서 학습되어야 할 파라미터의 개수는?
① 288개
② 816개
③ 784개
④ 320개

8. 경사하강법에서 학습률에 관한 설명으로 적절하지 못한 설명은?
① 학습률은 초기에 작은 값으로 시작하는 것이 유리하다.
② 학습률이 작으면 느리게 학습하며 시간이 오래 걸린다는 단점이 있다.
③ 학습률이 크면 최소값을 지나쳐버려 정확한 학습을 못할 수도 있다.
④ 학습률을 선택함에 있어 정확한 기준은 없다.

9. GAN과 Pix2Pix의 특징 설명 중 옳은 것은?
① GAN에는 한 개의 네트워크로 구성되어 있다.
② pix2pix는 이미지를 input으로 받아서 다른 style의 이미지를 output 한다.
③ Pix2pix 알고리즘은 random noise를 input으로 받는다.
④ GAN은 지도학습 기반의 알고리즘이다.

10. CNN에서 필터 사이즈 1x1을 적용하는 것으로서 Depth wise Convolution과 짝을 이루어 Separable Convolution에 활용되는 레이어는?
① Pointwise Convolution
② Separable Convolution
③ Transposed Convolution
④ Grouped Convolution

11. 자연어처리는 한 가지 단어가 여러 가지 뜻을 가지고 있어 분석하기 쉽지 않다. 이러한 언어의 특성은?
① 토큰화
② NER
③ 중의성
④ 형태소

12. 각 단어에 대한 중요도를 계산할 수 있는 방법은?
① DTM
② TF-IDF
③ SLM
④ LSA

13. 감성분석(sentiment analysis)에 대한 설명으로 맞지 않는 것은?
① 문장에서 사용된 긍정과 부정의 점수를 매긴다.
② 긍정사전과 부정사전은 모든 분야에서 공통으로 사용할 수 있다.
③ 긍정 단어 사전과 부정단어 사전이 필요하다.
④ 브랜드 평판의 추이를 분석할 수 있다.

14. RNN의 문제점 설명 중 잘못된 것은?
① 문장이 길어지면 기울기 소실이 발생할 수 있다.
② 문장이 길어지면 기울기 폭발이 발생할 수 있다.
③ 기울기 폭발은 기울기 클리핑 기법으로 방지할 수 있다.
④ RNN은 합성곱 연산을 수행하여 자연어의 특징을 추출한다.

15. BPE(Byte Pair Encoding) 알고리즘에 대한 설명으로 옳지 않은 것은?
① 신조어나 오타와 같은 UNK(Unknown) 토큰에 대한 효율적인 대처이다.
② 단어는 의미를 가진 더 작은 서브워드들의 조합으로 이루어진다는 가정하에 적용되는 알고리즘이다.
③ concentrate라는 영어 단어를 con(=together) + centr(=center) + ate(=make)의 서브워드로 나누어 분석한다.
④ BPE를 사용하면 어휘수를 줄일 수 있지만 희소성이 발생한다.

16. 시소러스(thesaurus)의 대한 설명으로 옳지 않은 것은?
① 시소러스는 자연어의 계층 구조를 분석하고 분류하여 구축한 데이터베이스이다
② 대표적인 시소러스는 워드넷(WordNet)이 있다.
③ 시소러스를 사용하여 상위어, 하위어에 대한 정보를 확인할 수 있다.
④ 시소러스로 동의어를 확인하는 것은 불가능하다.

17. 언어모델의 성능을 평가하는 척도로 문장의 길이를 반영하여 확률값을 정규화한 값은?
① Accuracy
② PPL(Perplexity)
③ Precision
④ IoU

18. 인코더-디코더 기반의 언어모델 중 하나로, 쿼리와 비슷한 값을 가진 키를 찾아서 그 값을 얻는 과정을 수행하는 구조는?
① Word2Vec
② FastText
③ Attention
④ Gensim

19. 트랜스포머(Transformer)에 대한 설명으로 옳지 않은 것은?
① Positional Encoding을 추가하여 텍스트의 상대적인 위치를 더하는 연산을 수행한다.
② Multi-head self-attention 모델로, attention 계층을 동시에 여러 개 수행한다.
③ RNN 기반의 언어모델이다.
④ Attention 계층의 입력으로 Query, Key, Value 값이 필요하다.

20. BERT 모델의 설명이 아닌 것은?
① BERT는 파인튜닝 기반 전이학습을 한다.
② 마스크된 언어 모델링 방법인 MLM을 사용한다.
③ 훈련단계에서 학습 수렴속도가 빠르다.
④ 양방향 RNN훈련이 가능하다.